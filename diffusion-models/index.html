<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      background-color: hsl(0, 0%, 6%);
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: "Open Sans", sans-serif;
      color: #c8c8c8;
      line-height: 1.5;
    }

    h1,
    h2,
    h3,
    h4,
    h5 {
      font-family: "Trebuchet MS", sans-serif;
      color: #e2e2e2;
    }

    a {
      color: #7caefe;
    }

    blockquote {
      color: #ffffff;
      background-color: #313131;
    }
  </style>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>



  <title>Diffusion Models</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>


  <h1 align="middle">Diffusion Models</h1>
  <h2 align="middle">Jaxon Zeng</h2>

  <br><br>

  <div>
    <h2 align="middle">Overview</h2>

    <p>This project is divided into two main components, each focusing on the innovative use of <strong>diffusion
        models</strong> for image-related tasks:</p>
    <ul>
      <li><strong>Part A: Utilizing a Pretrained Model</strong>
        <ul>
          <li>I experimented with a pretrained diffusion model called <strong><a
                href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a></strong>,
            sourced from Hugging Face, to generate various types of images. This part showcases how existing advanced
            models can transform text descriptions into visual content.</li>
        </ul>
      </li>
      <li><strong>Part B: Developing a Custom Model</strong>
        <ul>
          <li>I designed and trained my own diffusion model from scratch using the <code>torch.nn.Module</code>
            framework, with the <strong>MNIST dataset</strong> (a collection of handwritten digits) for training. This
            demonstrates the process of building tailored solutions for specific image processing tasks like denoising.
          </li>
        </ul>
      </li>
    </ul>


    <br><br>
    <h2 align="middle">Part A: The Power of Diffusion Models</h2>
    <h3 align="middle">Setup</h3>


    <h4>Model Overview</h4>
    <ul>
      <li><strong>Model Selection:</strong> For Part A, I utilize the pretrained <strong><a
            href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a></strong> model from
        Hugging Face. This advanced tool is designed to generate high-quality images from text prompts through a
        structured process.</li>
      <li><strong>Purpose:</strong> The aim is to demonstrate how diffusion models like DeepFloyd IF can transform
        textual descriptions into detailed visuals, making image creation accessible to non-experts.</li>
    </ul>
    <h4>Two-Stage Process</h4>
    <ul>
      <li><strong>Stage 1 - Initial Denoising:</strong> The first stage processes noisy images of size $64\times64$
        pixels, integrating them with text embeddings (text converted into a format the model can interpret) to produce
        a denoised, preliminary image. This forms the basis of the image generation.</li>
      <li><strong>Stage 2 - Resolution Enhancement:</strong> The output from the first stage is then refined in this
        stage, upscaling the image to a higher resolution of $256\times256$ pixels, resulting in sharper and more
        detailed visuals.</li>
    </ul>
    <h4>Diffusion Process Explained</h4>
    <ul>
      <li><strong>Forward Process:</strong> As the timestep variable $t$ increases, noise is incrementally added to the
        images, progressively obscuring their clarity. This simulates a degradation of the image over time.</li>
      <li><strong>Backward Process:</strong> Conversely, the backward process, primarily executed by the denoiser in
        Stage 1, focuses on estimating and removing the noise from the image. This step-by-step reconstruction is
        central to how diffusion models achieve clear, refined outputs.</li>
    </ul>
    <div align="middle">
      <img src="imagesa/ddpm_markov.png" align="middle" width="700px" />
    </div>
    For the whole part, I use random seed $24$ for reproducibility purpose. I also use the text embeddings of
    <code>"a high quality photo"</code> as a default text embeddings if not mentioned specifically.<br>
    <br>
    Here's some images generated from the model:
    <div align="middle">
      <table style="width=100%">
        <h5>Inference steps = 5</h5>
        <tr>
          <td align="middle">
            <img src="imagesa/1-1-1.png" align="middle" width="64px" />
            <figcaption align="middle">an oil painting of a snowy mountain<br>stage 1</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-1-2.png" align="middle" width="64px" />
            <figcaption align="middle">a man wearing a hat<br>stage 1</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-1-3.png" align="middle" width="64px" />
            <figcaption align="middle">a rocket ship<br>stage 1</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesa/1-1-4.png" align="middle" width="256px" />
            <figcaption align="middle">an oil painting of a snowy mountain<br> stage 2</figcaption>
          </td>
          <td>
            <img src="imagesa/1-1-5.png" align="middle" width="256px" />
            <figcaption align="middle">a man wearing a hat<br>stage 2</figcaption>
          </td>
          <td>
            <img src="imagesa/1-1-6.png" align="middle" width="256px" />
            <figcaption align="middle">a rocket ship<br>stage 2</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        <h5>Inference steps = 20</h5>
        <tr>
          <td align="middle">
            <img src="imagesa/1-2-1.png" align="middle" width="64px" />
            <figcaption align="middle">an oil painting of a snowy mountain<br>stage 1</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-2-2.png" align="middle" width="64px" />
            <figcaption align="middle">a man wearing a hat<br>stage 1</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-2-3.png" align="middle" width="64px" />
            <figcaption align="middle">a rocket ship<br>stage 1</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesa/1-2-4.png" align="middle" width="256px" />
            <figcaption align="middle">an oil painting of a snowy mountain<br> stage 2</figcaption>
          </td>
          <td>
            <img src="imagesa/1-2-5.png" align="middle" width="256px" />
            <figcaption align="middle">a man wearing a hat<br>stage 2</figcaption>
          </td>
          <td>
            <img src="imagesa/1-2-6.png" align="middle" width="256px" />
            <figcaption align="middle">a rocket ship<br>stage 2</figcaption>
          </td>
        </tr>
      </table>
    </div>


    <br>
    <br>
    <h3 align="middle">Forward Process</h3>
    Forward process in diffusion models is to add noise to clean images.
    The forward process algorithm is defined by:
    $$q(x_t|x_0)=N(x_t;\sqrt{\bar{\alpha}}x_0, (1-\bar{\alpha}_t)I)$$
    is equivalent to:
    $$x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon \space\text{ where }\epsilon\sim N(0,1)$$
    $x_t$: noisy images<br>
    $x_0$: clean images<br>
    $\epsilon$: noise<br>
    $\bar{\alpha}_t$: <code>alpha_cumprod</code>, determined by the trainer of the model
    <br>
    <br>

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="230px" />
            <figcaption align="middle">Berkeley Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-2.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=250$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-3.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=500$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-4.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=750$</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>

    <h3 align="middle">Classical Denoising</h3>
    Classically, we use gaussian blur filter to get rid of noise. But in this case this classical denoising does
    not work well.

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-4-1.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=250$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-4-2.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=500$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-4-3.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=750$</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-4-4.png" align="middle" width="230px" />
            <figcaption align="middle">Gaussian Denoised Campanile<br>at $t=250$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-4-5.png" align="middle" width="230px" />
            <figcaption align="middle">Gaussian Denoised Campanile<br>at $t=500$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-4-6.png" align="middle" width="230px" />
            <figcaption align="middle">Gaussian Denoised Campanile<br>at $t=750$</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>



    <h3 align="middle">One-Step Denoising</h3>
    One step denoising uses the pretrained diffusion model to denoise. The denoiser located at
    <code>stage_1.unet</code>. This denoiser estimates the noise in the noisy images given the timestep. Then remove the
    noise from noisy images can recover the estimate of original images.

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="230px" />
            <figcaption align="middle">Berkeley Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="230px" />
            <figcaption align="middle">Berkeley Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="230px" />
            <figcaption align="middle">Berkeley Campanile</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-5-1.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=250$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-5-2.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=500$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-5-3.png" align="middle" width="230px" />
            <figcaption align="middle">Noisy Campanile at $t=750$</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-5-4.png" align="middle" width="230px" />
            <figcaption align="middle">One-Step Denoised Campanile<br>at $t=250$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-5-5.png" align="middle" width="230px" />
            <figcaption align="middle">One-Step Denoised Campanile<br>at $t=500$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-5-6.png" align="middle" width="230px" />
            <figcaption align="middle">One-Step Denoised Campanile<br>at $t=750$</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>



    <h3 align="middle">Iterative Denoising</h3>


    <h4>Enhancing Image Clarity through Iterative Denoising</h4>
    <ul>
      <li><strong>Comparison of Denoising Methods:</strong> It's clear that the <strong>Unet denoiser</strong>
        outperforms the traditional Gaussian denoiser in refining image quality. However, challenges remain as images
        become blurrier with increasing levels of added noise.</li>
      <li><strong>Solution - Iterative Denoising:</strong> To address this issue and further improve performance, I
        implemented an <strong>iterative denoising</strong> approach. This method refines the image step by step,
        tackling noise more effectively than a single-pass process.</li>
    </ul>
    <h4>Technical Approach to Iterative Denoising</h4>
    <ul>
      <li><strong>Theoretical Basis:</strong> In theory, diffusion models like DeepFloyd IF allow for iterative
        denoising, where each step incrementally removes noise to reveal a clearer image.</li>
      <li><strong>Optimized Implementation:</strong> To save time while maintaining effectiveness, I used a
        <strong>stride of 30</strong> within the total timestep of $T=1000$. This means the process skips intermediate
        steps, focusing on key intervals for efficiency.
      </li>
      <li><strong>Timestep Sequence:</strong> I generated a specific list of timesteps called
        <code>strided_timesteps</code> to guide the iterative process. The sequence includes the following values:
        <code>[990, 960, 930, 900, 870, 840, 810, 780, 750, 720, 690, 660, 630, 600, 570, 540, 510, 480, 450, 420, 390, 360, 330, 300, 270, 240, 210, 180, 150, 120, 90, 60, 30, 0]</code>.
        This structured progression ensures a balanced approach between speed and image quality.
      </li>
    </ul>
    <br>
    <br>
    The denoising algorithm on the <code>i</code>th step is:
    $$x_{t'}=\frac{\sqrt{\bar{\alpha}_{t'}}\beta_t}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t'})}{1-\bar{\alpha}_t}x_t+v_\sigma$$
    $t$: time at <code>strided_timesteps[i]</code><br>
    $t'$: time at <code>strided_timesteps[i+1]</code><br>
    $x_t$: image at timestep $t$<br>
    $x_{t'}$: image at timestep $t'$<br>
    $\bar{\alpha}_t$: <code>alpha_cumprod</code><br>
    $\alpha_t$: $\frac{\bar{\alpha}_t}{\bar{\alpha}_{t'}}$<br>
    $\beta_t$: $1-\alpha_t$<br>
    $x_0$: the current estimate of clean image as in the one-step denoising<br>
    $v_\sigma$: random noise

    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-6-1.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=90$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-2.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=240$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-3.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-4.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=540$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-5.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-6.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-6-7.png" align="middle" width="100px" />
            <figcaption align="middle">Noisy Campanile at $t=990$</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    In this part, I use <code>i_start = 10</code>, which correspondence to timestep 690.
    <br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="200px" />
            <figcaption align="middle">Berkeley <br>Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-7-1.png" align="middle" width="200px" />
            <figcaption align="middle">Noisy Campanile <br>at $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-7-2.png" align="middle" width="200px" />
            <figcaption align="middle">Iterative Denoised Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-7-3.png" align="middle" width="200px" />
            <figcaption align="middle">One-Step Denoised Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-7-4.png" align="middle" width="200px" />
            <figcaption align="middle">Gaussian Denoised Campanile</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>

    <h3 align="middle">Diffusion Model Sampling</h3>
    By taking <code>i_start = 0</code>, the algorithm would denoise from pure noise and start generate images.
    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-8-1.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 1</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-8-2.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 2</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-8-3.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 3</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-8-4.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 4</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-8-5.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 5</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>


    <h3 align="middle">Classifier-Free Guidance (CFG)</h3>
    Some images generated in the previous section are not very good. To improve the quality of images, I use a
    technique called <a href="https://arxiv.org/abs/2207.12598">Classifier-Free Guidance</a>. In this technique, the
    algorithm computes both a conditional and an unconditional noise estimate, then the new noise will be:
    $$\epsilon=\epsilon_u+\gamma(\epsilon_c+\epsilon_u)$$
    By taking $\gamma>1$, we get a much higher quality images. For this and later sections, I use text embeddings
    <code>""</code> as the unconditional prompt and <code>"a high quality photo"</code> as the conditional prompt.
    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-9-1.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 1 with CFG</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-9-2.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 2 with CFG</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-9-3.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 3 with CFG</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-9-4.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 4 with CFG</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-9-5.png" align="middle" width="200px" />
            <figcaption align="middle">Sample 5 with CFG</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    The results compare to the previous section is much more vivid and high-contrast.
    <br>
    <br>

    <h3 align="middle">Image-to-Image Translation</h3>
    <ul>
      <li><strong>What It Is:</strong> Image-to-image translation is a technique that starts with a clean image,
        introduces a controlled amount of noise to it, and then removes that noise through a denoising process. This
        method enables modifications to existing images.</li>
      <li><strong>Impact of Noise Levels:</strong> The extent of the edit depends on the amount of noise added. The more
        noise introduced, the more significant the changes to the final image, allowing for a range of subtle to
        dramatic transformations.</li>
    </ul>

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-10-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-10-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-10-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-10-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-10-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-10-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="150px" />
            <figcaption align="middle">Berkeley Campanile</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-11-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-11-0.png" align="middle" width="150px" />
            <figcaption align="middle">Cat Meme</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-12-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-12-0.png" align="middle" width="150px" />
            <figcaption align="middle">Sad Meme</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>

    <h4 align="middle">Editing Hand-Drawn and Web Images</h4>
    Except for taking in realistic images, the algorithm can also edit hand-drawn and web images.
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-13-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-13-0.png" align="middle" width="150px" />
            <figcaption align="middle">Web Img</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-14-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-14-0.png" align="middle" width="150px" />
            <figcaption align="middle">Hand-Drawn 1</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-15-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit with $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-15-0.png" align="middle" width="150px" />
            <figcaption align="middle">Hand-Drawn 2</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <h4 align="middle">Inpainting</h4>
    Given an image and a mask, I can also generate images that only the masked area changes while other area stays the
    same. In each loop, the new image will be:
    $$x_t\leftarrow mx_t+(1-m)\text{forward}(x_{orig},t)$$
    $x_{orig}$: original image<br>
    $m$: binary mask
    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-16-1.png" align="middle" width="150px" />
            <figcaption align="middle">Campanile</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-16-2.png" align="middle" width="150px" />
            <figcaption align="middle">Mask</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-16-3.png" align="middle" width="150px" />
            <figcaption align="middle">Hole to Fill</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-16-4.png" align="middle" width="150px" />
            <figcaption align="middle">Campanile Filled</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-17-1.png" align="middle" width="150px" />
            <figcaption align="middle">Cat</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-17-2.png" align="middle" width="150px" />
            <figcaption align="middle">Mask</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-17-3.png" align="middle" width="150px" />
            <figcaption align="middle">Hole to Fill</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-17-4.png" align="middle" width="150px" />
            <figcaption align="middle">Cat Filled</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-18-1.png" align="middle" width="150px" />
            <figcaption align="middle">Sunset</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-18-2.png" align="middle" width="150px" />
            <figcaption align="middle">Mask</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-18-3.png" align="middle" width="150px" />
            <figcaption align="middle">Hole to Fill</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-18-4.png" align="middle" width="150px" />
            <figcaption align="middle">Sunset Filled</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <h4 align="middle">Text-Conditional Image-toimage Translation</h4>
    In this section, I experiments image editing by changing different text prompt from
    <code>"a high quality photo"</code>.
    <br>
    <br>
    <div align="middle">
      <table style="width=100%">
        prompt = <code>"a rocket ship"</code>
        <tr>
          <td align="middle">
            <img src="imagesa/1-19-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-19-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-19-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-19-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-19-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-19-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-3-1.png" align="middle" width="150px" />
            <figcaption align="middle">Campanile to Rocket Ship</figcaption>
          </td>
        </tr>
      </table>
      <br>
      <table style="width=100%">
        prompt = <code>"a photo of a man"</code>

        <tr>
          <td align="middle">
            <img src="imagesa/1-20-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-20-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-20-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-20-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-20-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-20-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-17-1.png" align="middle" width="150px" />
            <figcaption align="middle">Cat to <br>Man </figcaption>
          </td>
        </tr>
      </table>
      <br>
      <table style="width=100%">
        prompt = <code>"a photo of a hipster barista"</code>
        <tr>
          <td align="middle">
            <img src="imagesa/1-21-1.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=960$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-21-2.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=900$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-21-3.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=840$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-21-4.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=780$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-21-5.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=690$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-21-6.png" align="middle" width="150px" />
            <figcaption align="middle">Edit <br> $t=390$</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-18-1.png" align="middle" width="150px" />
            <figcaption align="middle">Sunset to Hipster Barista</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <h3 align="middle">Visual Anagrams</h3>

    In this section, I explore the creation of optical illusions using diffusion models. These illusions are designed to
    generate images that appear as one object when viewed normally and as a completely different object when viewed
    upside down.</li>

    <h4>Implementation Method</h4>
    <ul>
      <li><strong>Modifying Noise Estimation:</strong> To achieve this dual-image effect, the noise estimation process
        of the diffusion model is adjusted. This involves combining two different noise predictions to create a hybrid
        image that holds two distinct interpretations based on orientation.</li>
      <li><strong>Algorithm Details:</strong> The specific approach uses the following steps to modify the noise
        estimation:
        <ul>
          <li>Compute the first noise estimate using the UNet model with the original image and a specific prompt.</li>
          $$\epsilon_1 = \text{UNet}(x_t, t, p_1)$$
          <li>Compute the second noise estimate by flipping the image, applying the UNet model with a different prompt,
            and flipping the result back.</li>
          $$\epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2))$$
          <li>Average the two noise estimates to create the final noise value used in the denoising process.</li>
          $$\epsilon = (\epsilon_1 + \epsilon_2)/2$$
        </ul>
      </li>
    </ul>

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-22-1.png" align="middle" width="200px" />
            <figcaption align="middle">an old painting of <br>people around a campfire</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-22-2.png" align="middle" width="200px" />
            <figcaption align="middle">an old painting of <br>an old man</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-23-1.png" align="middle" width="200px" />
            <figcaption align="middle">a rocket ship</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-23-2.png" align="middle" width="200px" />
            <figcaption align="middle">a pencil</figcaption>
          </td>
        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-24-1.png" align="middle" width="200px" />
            <figcaption align="middle">a photo of a dog</figcaption>
          </td>
          <td align="middle">
            <img src="imagesa/1-24-2.png" align="middle" width="200px" />
            <figcaption align="middle">a photo of a man</figcaption>
          </td>
        </tr>
      </table>
    </div>


    <br>
    <br>
    <h3 align="middle">Hybrid Images</h3>

In this section, I explore the creation of a unique optical illusion using diffusion models. These hybrid images are designed to appear as one object when viewed up close and as a completely different object when viewed from a distance.</li>

<h4>Implementation Method</h4>
<ul>
    <li><strong>Modifying Noise Estimation:</strong> To achieve this dual-perception effect, the noise estimation process in the diffusion model is customized. This involves blending two distinct noise predictions to craft an image that reveals different content based on proximity.</li>
    <li><strong>Algorithm Details:</strong> The specific approach modifies the noise estimation using the following steps:
        <ul>
            <li>Compute the first noise estimate with the UNet model using the image at a given timestep and a specific prompt.</li>
            $$\epsilon_1 = \text{UNet}(x_t, t, p_1)$$
            <li>Compute the second noise estimate similarly with a different prompt.</li>
            $$\epsilon_2 = \text{UNet}(x_t, t, p_2)$$
            <li>Combine the two estimates by applying a low-pass filter to the first and a high-pass filter to the second, then summing them. This technique ensures that low-frequency details (visible from afar) and high-frequency details (visible up close) create the dual-image effect.</li>
            $$\epsilon = f_\text{lowpass}(\epsilon_1) + f_\text{highpass}(\epsilon_2)$$
        </ul>
    </li>
</ul>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td align="middle">
            <img src="imagesa/1-25-1.png" align="middle" width="200px" />
            <figcaption align="middle">Hybrid image of a skull and a waterfall</figcaption>
          </td>

        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-25-2.png" align="middle" width="200px" />
            <figcaption align="middle">Hybrid image of a waterfall and a dog</figcaption>
          </td>

        </tr>
        <tr>
          <td align="middle">
            <img src="imagesa/1-25-3.png" align="middle" width="200px" />
            <figcaption align="middle">Hybrid image of an old man and a rocket ship</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <br>
    <br>




    <h2 align="middle">Part B: Diffusion Models from Scratch</h2>
In this part of the project, I focus on constructing a denoising UNet model from the ground up using the <code>torch.nn</code> framework, a powerful tool for building neural networks.
<h4>Overview of Custom UNet Development</h4>
<ul>
    <li><strong>Training Data:</strong> For training this model, I utilize the MNIST dataset, accessed through <code>torchvision.datasets.MNIST</code>. This dataset consists of thousands of handwritten digit images, providing a solid foundation for teaching the model to recognize and reconstruct numbers.</li>
    <li><strong>Purpose:</strong> The goal is to demonstrate the process of creating a custom diffusion model tailored for denoising tasks, showing how such models can be built and trained to refine noisy images into clear, recognizable digits.</li>
</ul>

    <br>
    For the whole part, I use random seed $24$ for reproducibility purpose.

    <br>
    <br>
    <h3 align="middle">Unconditional UNet</h3>
    The Unconditional UNet structure is:<br>
    <div align="middle">
      <img src="imagesb/unet3.png" align="middle" width="900px" />
    </div>
    The standard tensor operations is defined as:<br>
    <div align="middle">
      <img src="imagesb/unet_atomic_ops.png" align="middle" width="900px" /><br>
    </div>


    For forward process, I will add noise to image according to $\sigma$ level. The noise algorithm is:
    $$z=x+\sigma\epsilon,\text{ where }\epsilon\sim N(0, I)$$

    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-1-1.png" align="middle" width="800px" />
            <figcaption align="middle">Various Noise Level</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>

    The number of hidden layers is $128$ for unconditional UNet. <br>
    I train the model using noisy image $z$ with $\sigma=0.5$
    applied to clean images $x$. The batch size is $256$ and number of epoch is $5$. I choose Adam optimizer with
    initial learning rate of $1e-4$<br>
    I train the model using an L2 loss:
    $$L=\mathbb{E}_{z,x}\lVert D_\theta(z)-x\rVert^2$$
    The training loss is:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-1-2.png" align="middle" width="600px" />
            <figcaption align="middle">Unconditional UNet Traning Loss</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    The denoise results of the training in different epoch are:<br>
    <div align="middle">(top: original image, middle: noisy image, bottom:
    estimate original image)</div>
    
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-1-3.png" align="middle" width="800px" />
            <figcaption align="middle">Unconditional UNet Denoise Result Epoch 1</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/2-1-4.png" align="middle" width="800px" />
            <figcaption align="middle">Unconditional UNet Denoise Result Epoch 5</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    I also test the model for out of distribution noise levels:
    <div align="middle">
      <table style="width=100%">

        <tr>
          <td>
            <img src="imagesb/2-1-5.png" align="middle" width="800px" />
            <figcaption align="middle">Out of Distribution Noise Levels Denoise Result</figcaption>
          </td>
        </tr>
      </table>
    </div>


    <br>
    <br>

    <h3 align="middle">Time Conditional UNet</h3>

To create a diffusion model similar to the one used in Part A, I needed to incorporate the concept of time into the model to enable iterative denoising. This allows the model to refine images step by step over multiple time intervals. Introducing time as a variable helps the model understand the progression of noise addition and removal, mimicking the forward and backward processes of diffusion models for more effective image reconstruction.

    <h4>Implementation Approach</h4>
<ul>
    <li>To achieve this, I added two <strong>fully connected blocks</strong> to the model architecture by the following structure. These blocks integrate the time step <code>t</code> as a parameter, guiding the model on how much noise to remove at each stage of the denoising process.</li>
</ul>

    <div align="middle">
      <img src="imagesb/conditional_arch.png" align="middle" width="900px" />
    </div>
    The fully-connected block is defined as:<br>
    <div align="middle">
      <img src="imagesb/fc_long.png" align="middle" width="700px" /><br>
    </div>
    <br>

    The forward process (adding noise) in this model is changed to:
    $$x_t = \sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\space\text{ where }\space\epsilon\sim N(0,
    1)\space\text{for}\space t\in\{0,1,...,T\}$$
    Some parameters are precomputed in the UNet. The computation according to <a
      href="https://arxiv.org/abs/2006.11239">DDPM paper</a> is:<br>
    $\beta_t$: a list of $\beta$ of length $T+1$ such that $\beta_0=0.0001$ and $\beta_T=0.02$ and all other elements
    $\beta_t$ for $t\in\{1,...,T-1\}$ are evenly spaced between the two.<br>
    $\alpha_t$: $1-\beta_t$<br>
    $\bar{\alpha}_t$: $\prod_{s=1}^t{\alpha_s}$ is a cumulative product of $a_s$ for $s\in\{1,...,t\}$<br>
    <br>
    The training algorithm of the model is:
    <div align="middle">
      <img src="imagesb/algo1_t_only.png" align="middle" width="800px" />
    </div>
    <br>
    The denoising algorithm of the model is:<br>
    <div align="middle">
      <img src="imagesb/algo2_t_only.png" align="middle" width="800px" /><br>
    </div>
    <br>
    <br>
    For this UNet model, the number of hidden layers is $64$. The total timestep $T=300$.<br>
    For training, I uses a batch size of $128$ and number of epoch as $20$. I still use the Adam optimizer with an
    initial learning rate of $1e-3$. I also set an exponential learning rate decay scheduler with a gamma of
    $0.1^{(1.0/\text{num_epochs})}$ by calling <code>torch.optim.lr_scheduler.ExponentialLR</code>.<br>
    I still use L2 loss in training:
    $$L=\mathbb{E}_{\epsilon, x_0,t}\lVert\epsilon_\theta(x_t,t)-\epsilon\rVert ^2$$
    <br>
    The training loss is:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-2-2.png" align="middle" width="600px" />
            <figcaption align="middle">Time Conditional UNet Traning Loss</figcaption>
          </td>
        </tr>

      </table>
    </div>
    <br>
    The denoise results of the training in different epoch are:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/time_cond_epoch1.gif" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Denoise Result Epoch 1</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/time_cond_epoch5.gif" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Denoise Result Epoch 5</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/time_cond_epoch10.gif" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Denoise Result Epoch 10</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/time_cond_epoch15.gif" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Denoise Result Epoch 15</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/time_cond_epoch20.gif" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Denoise Result Epoch 20</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    The denoised output after training is:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-2-3.png" align="middle" width="900px" />
            <figcaption align="middle">Time Conditional UNet Test Denoised Result</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <br>
    <h3 align="middle">Class Contidional UNet</h3>

    <h4>Performance Issue</h4>
It's evident that the denoised output from the time conditional model does not always produce high-quality digit images. While it incorporates time as a variable for iterative denoising, the results still fall short in accurately generating clear digits.
<br>
I improved the model by integrating a <strong>class condition</strong> to provide additional context about the type of digit being generated. This helps the model focus on specific digit characteristics during the denoising process.

    <h4>Implementation Approach</h4>
<ul>
    <li>To achieve this, I added two additional <strong>fully-connected blocks</strong> to the model architecture. These blocks are placed at the same locations as the existing time condition blocks, allowing the model to process both time and class information simultaneously.</li>
    <li>By conditioning the model on class labels (e.g., identifying whether the image should be a '1', '2', etc.), it can better tailor the denoising process, resulting in improved digit generation quality.</li>
</ul>
    <br>
    <br>
    The training algorithm of the model is:
    <div align="middle">
      <img src="imagesb/algo3_c.png" align="middle" width="800px" />
    </div>
    <br>
    The denoising algorithm of the model is:<br>
    <div align="middle">
      <img src="imagesb/algo4_c.png" align="middle" width="800px" /><br>
    </div>
    <br>



The model parameters and training process for the class-conditional UNet are largely consistent with those of the time-conditional UNet model. This ensures a comparable foundation in terms of architecture and training methodology.<br>
The focus remains on enhancing the model's ability to denoise and generate images, with the added benefit of class-specific guidance to improve the accuracy of digit reconstruction.
</ul>
<h4>Class-Conditioning Vector Implementation</h4>
<ul>
    <li><strong>One-Hot Encoding:</strong> For the class-conditioning vector $c$, I utilize one-hot encoding to transform the labels from the dataset into a one-hot vector format. This encoding allows the model to recognize and focus on specific digit classes (e.g., '0' through '9') during the denoising process.</li>
    <li><strong>Dropout Mechanism:</strong> Since the UNet must sometimes operate without class conditioning to maintain flexibility, I implemented a dropout mechanism. This dropout is applied at a probability of $10%$, where the class-conditioning vector is set to zero, effectively disabling class guidance in those instances.</li>
</ul>
<br>
    The training loss is:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-3-2.png" align="middle" width="600px" />
            <figcaption align="middle">Class Conditional UNet Traning Loss</figcaption>
          </td>
        </tr>
      </table>
    </div>

    The denoise results of the training in different epoch are:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/class_cond_epoch1.gif" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Denoise Result Epoch 1</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/class_cond_epoch5.gif" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Denoise Result Epoch 5</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/class_cond_epoch10.gif" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Denoise Result Epoch 10</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/class_cond_epoch15.gif" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Denoise Result Epoch 15</figcaption>
          </td>
        </tr>
        <tr>
          <td>
            <img src="imagesb/class_cond_epoch20.gif" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Denoise Result Epoch 20</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    The denoised output after training is:
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="imagesb/2-3-3.png" align="middle" width="900px" />
            <figcaption align="middle">Class Conditional UNet Test Denoised Result</figcaption>
          </td>
        </tr>
      </table>
    </div>


</body>

</html>